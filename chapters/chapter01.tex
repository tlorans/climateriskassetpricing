\chapter{From Text to Data}


Text is high dimensional. 
Suppose we have a bunch of documents, each of 
which is $w$ words long. Each word is 
drawn from a vocabulary of $p$ possible 
words. The unique representation of these 
documents has dimension $p^w$. 

Analysis can 
summarized in three steps:

\begin{enumerate}
    \item Represent raw text $D$ as a numerical 
    array $C$
    \item Map $C$ to predicted 
    values $\hat{V}$ of unknown outcomes 
    $V$
    \item Use $\hat{V}$ in subsequent analysis
\end{enumerate}


\section{Representing Text as Data}
 

The first step in constructing $C$ is 
to divide the raw text $D$ into individual documents 
$D_i$. The way to divide the raw text 
is dictated by the value of interest $V$. 
If $V$ is daily stock price, it might makes sense
to divide the raw text into daily news articles.
To reduce the number of features, 
a common first step is to strip out elements 
of $D_i$ that are not words (punctuation,
numbers, etc.). It is also common to remove common and rare words.

To begin with the transformation from raw text $D$ to
a numerical array $C$, we can first count 
the number of times each word appears in each document, $c_{i,j}$.
This is the \textit{term frequency} of word $j$ in document $i$. You 
may want to normalize this count by the total number of words in the document:

\begin{equation}
tf_{i,j} = \frac{c_{i,j}}{n_i}
\end{equation}

where $n_i$ is the total number of words in document $i$.
It results into a matrix $C$ of size $n \times p$ where $n$ is the number of documents and $p$ is 
the number of unique words in the vocabulary.
Each row of $C$ refers to a document $i$, and each column refers to a word $j$.


\begin{examplebox}
    \textbf{Example X.1.} 
\begin{table}[H]
    \centering
    \begin{tabular}{|c|p{5cm}|}
        $D_1$ & a rose is still a rose \\
        $D_2$ & there is no there there \\
        $D_3$ & rose is a rose is a rose is a rose \\
    \end{tabular}
    \caption{Examples of individual documents $D_i$}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
        $i\setminus j$ & a & rose & is & still & there & no \\
        \hline
        $1$ & 2 & 2 & 1 & 1 & 0 & 0 \\
        $2$ & 0 & 0 & 1 & 0 & 3 & 1  \\
        $3$ & 3 & 4 & 3 & 0 & 0 & 0  \\
    \end{tabular}
    \caption{Term frequency matrix $C$}
\end{table}

If we use the normalized term frequency, we have
for example:
$tf_{1,rose} = \frac{2}{6} = \frac{1}{3}$ 

and $c_1$ is the row vector:

\begin{equation}
    c_1 = \begin{bmatrix}
        \frac{2}{6} & \frac{2}{6} & \frac{1}{6} & \frac{1}{6} & 0 & 0
    \end{bmatrix}
\end{equation}
\end{examplebox}

So far, we have only counted 
the number of times each word appears in each document.
The problem is that common words like "the" or "and"
appear in many documents and do not provide much information.
One way to address this is to compute the 
\textit{inverse document frequency} (idf) of each word $j$.
This is the log of the ratio of the total number of documents $n$ 
to the number of documents $d_j$ 
that contain word $j$:

\begin{equation}
idf_{j} = \log (\frac{n}{d_j})
\end{equation}

where $d_j$ is the number of documents that contain word $j$.

We finally combine the term frequency and the inverse document frequency
to get the \textit{term frequency-inverse document frequency} (tf-idf).

\begin{equation}
tf_{i,j} - idf_{j} = \frac{c_{i,j}}{n_i} \times \log (\frac{n}{d_j})
\end{equation}

Very rare words will have a low $tf-idf$, because
they have a low $tf$. Very common words 
will have a low $tf-idf$ because they have a low $idf$.

\begin{examplebox}
    \textbf{Example X.1.} 
    Continuing from the previous example,
    we have $d_{rose} = 2$ since the word "rose" appears in
    documents $D_1$ and $D_3$.
    The idf of "rose" is then:
    \begin{equation}
        idf_{rose} = \log (\frac{3}{2})
    \end{equation}

    The tf-idf of "rose" in document $D_1$ is then:
    \begin{equation}
        tf_{1,rose} \times idf_{rose} = \frac{1}{3} \times \log (\frac{3}{2})
    \end{equation}

    Applying this to all words in all documents, we get the tf-idf matrix $C$.

\end{examplebox}

\section{Mapping Data to Predicted Values}

Explaining cosine similarity




\section{Climate Attention Index}


\section{Further Reading}
Use of text:
\textit{Text as Data} by Gentzkow \textit{et al.} (2019) \cite{gentzkow2019text}
\textit{Narrative Asset Pricing} by Bybee \textit{et al.} (2023) \cite{bybee2023narrative}

Climate series:
Engle \textit{et al.} (2020) \cite{engle2020hedging}
Apel \textit{et al.} (2023) \cite{apel2023real}
\section{Exercises}

\section{Solutions}

\section{Python Project}

